\documentclass{article}

\usepackage[final]{nips}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\newcounter{question}
\newcommand{\question}{\stepcounter{question}\paragraph{Question \thequestion}}
\newcommand{\mat}[1]{\textbf{\textit{#1}}}

\title{Coursework 1 - Modelling}

\author{
  Jonas Osborn \\
  \texttt{jo14944} \\
  \And
  Tristan Saunders \\
  \texttt{ts16802} \\
  \And
  Corin Varney \\
  \texttt{cv14985} \\
}

\begin{document}

\maketitle

\section{The prior}
\subsection{Theory}
\question \emph{1.} As the instances of $y$ are noisy observations of the underlying process and we do not know anything about this uncertainty we can assume it is the sum of independent and identically distributed errors. The Central Limit Theorem states the distribution of the sum of a large enough number of independent, identically distributed variables will be approximately normally distributed. From this we can say that our model of $y$ has the following form:
\begin{align*}
	y & = f(x) + \epsilon \\
	\text{where:~}
	\epsilon & \sim \mathcal{N}(0, \mat{I})
\end{align*}
And so from this we have the likelihood of each $y_i$ as a Gaussian distribution with mean $f(x_i)$.\\

\emph{2.} Choosing a spherical covariance matrix for the likelihood means that we are assuming the different dimensions of $y_i$ to be independent and identically distributed. As they are independent they do not covary with each other and so the covariance matrix is diagonal. As they are identically distributed they all have the same variance. Therefore the covariance matrix is spherical.
\question
\begin{align*}
	p(\mat{Y}|f,\mat{X}) &= p(\mat{y}_N|\mat{y}_{N-1},\ldots,\mat{y}_1,f,\mat{X})p(\mat{y}_{N-1}|\mat{y}_{N-2},\ldots,\mat{y}_1,f,\mat{X})\ldots p(\mat{y}_1|f,\mat{X}) \\
\end{align*}
\subsubsection{Linear regression}
\question
\begin{align*}
	p(\mat{Y}|\mat{X},\mat{W}) &= \prod_{i}^Np(\mat{y}_i|\mat{x}_i,\mat{W}) \\
	p(\mat{Y}|\mat{X},\mat{W}) &= \prod_{i}^N\mathcal{N}(\mat{W}\mat{x}_i,\sigma^2\mat{I})
\end{align*}
\question A conjugate prior is one that is conjugate to the posterior, meaning they are in the same family of distributions. A conjugate prior is useful as it gives a closed-form solution for the posterior. If we didn't choose a conjugate prior then numerical integration may be necessary to calculate the posterior which may mean the solution is potentially intractable. The conjugate prior for a Gaussian posterior is a Gaussian.
\question Just as encoding the prefrence in a $L_2$ norm is equivalent to having a Gaussian prior, encoding the preference using a $L_1$ norm is equivalent to having a Laplace prior. This is because the Laplace distribution estimates median rather than the mean estimated by the Guassian and median minimises the $L_1$ norm and mean the $L_2$

The shape of the laplace distribution's probability density function, with it's higher peak around zero compared to the probability density function of a Gaussian means that more co-efficients are likely to be equal to zero and this leads to a sparser model than those produced by a Gaussian prior.
\question todo
\subsubsection{Non-parametric regression}
\question todo
\question todo
\question todo
\question todo
\question todo
\subsection{Practical}
\subsubsection{Linear regression}
\question todo
\subsubsection{Non-parametric regression}
\question todo
\question todo
\section{The posterior}
\subsection{Theory}
\subsubsection{Learning}
\subsubsection{Practical optimisation}
\subsubsection{Non-parametric}
\subsection{Practical}
\subsubsection{Linear representation learning}
\end{document}
