\documentclass{article}

\usepackage[final]{nips}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subcaption}
\usetikzlibrary{bayesnet}
\graphicspath{ {images/} }

\newcounter{question}
\newcommand{\question}{\stepcounter{question}\paragraph{Question \thequestion}}
\newcommand{\mat}[1]{\textbf{\textit{#1}}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\argmin}{arg\,min}

\title{Coursework 1 - Modelling}

\author{
  Jonas Osborn \\
  \texttt{jo14944} \\
  \And
  Tristan Saunders \\
  \texttt{ts16802} \\
  \And
  Corin Varney \\
  \texttt{cv14985} \\
}

\begin{document}

\maketitle

\section{The prior}
\subsection{Theory}
\question \emph{1.} As the instances of $y$ are noisy observations of the underlying process and we do not know anything about this uncertainty we can assume it is the sum of independent and identically distributed errors. The Central Limit Theorem states the distribution of the sum of a large enough number of independent, identically distributed variables will be approximately normally distributed. From this we can say that our model of $y$ has the following form:
\begin{align*}
	y & = f(x) + \epsilon \\
	\text{where:~}
	\epsilon & \sim \mathcal{N}(0, \mat{I})
\end{align*}
And so from this we have the likelihood of each $y_i$ as a Gaussian distribution with mean $f(x_i)$.\\

\emph{2.} Choosing a spherical covariance matrix for the likelihood means that we are assuming the different dimensions of $y_i$ to be independent and identically distributed. As they are independent they do not covary with each other and so the covariance matrix is diagonal. As they are identically distributed they all have the same variance. Therefore the covariance matrix is spherical.
\question
\begin{align*}
	p(\mat{Y}|f,\mat{X}) &= p(y_1,\ldots,y_{N-1},y_N|f,\mat{X}) \\
	&= p(y_N|y_{N-1},\ldots,y_1,f,\mat{X})p(y_{N-1}|y_{N-2},\ldots,y_1,f,\mat{X})\ldots p(y_1|f,\mat{X}) \\
\end{align*}
\subsubsection{Linear regression}
\question
\begin{align*}
	p(\mat{Y}|\mat{X},\mat{W}) &= \prod_{i}^Np(\mat{y}_i|\mat{x}_i,\mat{W}) \\
	&= \prod_{i}^N\mathcal{N}(\mat{W}\mat{x}_i,\sigma^2\mat{I}) \\
	&= \mathcal{N}(\mat{W}\mat{X},\sigma^2\mat{I})
\end{align*}
\question A conjugate prior is one that is conjugate to the posterior, meaning they are in the same family of distributions. A conjugate prior is useful as it gives a closed-form solution for the posterior. If we didn't choose a conjugate prior then numerical integration may be necessary to calculate the posterior which may mean the solution is potentially intractable. The conjugate prior for a Gaussian posterior is a Gaussian.
\question Just as encoding the prefrence in a $L_2$ norm is equivalent to having a Gaussian prior, encoding the preference using a $L_1$ norm is equivalent to having a Laplace prior. This is because the Laplace distribution estimates median rather than the mean estimated by the Guassian and median minimises the $L_1$ norm and mean the $L_2$

The shape of the laplace distribution's probability density function, with it's higher peak around zero compared to the probability density function of a Gaussian means that more co-efficients are likely to be equal to zero and this leads to a sparser model than those produced by a Gaussian prior.
\question
\begin{align*}
	p(\mat{W}|\mat{X},\mat{Y}) &= \frac{1}{Z}p(\mat{Y}|\mat{X},\mat{W})p(\mat{W}) \\
	&= \frac{1}{Z}\mathcal{N}(\mat{W}\mat{X},\sigma^2\mat{I})\mathcal{N}(\mat{W}_0,\tau^2\mat{I})
\end{align*}
$\frac{1}{Z}$ is the normalising constant to ensure that the posterior is a probability density function by making the area under the graph equal to $1$ this constant is called the evidence. We will ignore it for now.

By the probability density function of the multivariate normal distribution, we have:
\begin{align*}
	p(\mat{W}|\mat{X},\mat{Y}) \propto\ &\frac{1}{\sqrt{(2\pi)^N\sigma^2}}\exp\bigg(-\frac{1}{2\sigma^2}(\mat{Y}-\mat{X}\mat{W})^T(\mat{Y}-\mat{X}\mat{W})\bigg) \\
	&\cdot \frac{1}{\sqrt{(2\pi)^N\tau^2}}\exp\bigg(-\frac{1}{2\tau^2}(\mat{W}-\mat{W}_0)^T(\mat{W}-\mat{W}_0)\bigg) \\
\end{align*}
We ignore the normalising constants as we re-normalise with $Z$ and then combine and multiply out the exponents.
\begin{align*}
	p(\mat{W}|\mat{X},\mat{Y}) \propto\ &\exp\bigg(-\frac{1}{2\sigma^2}(\mat{Y}-\mat{X}\mat{W})^T(\mat{Y}-\mat{X}\mat{W})-\frac{1}{2\tau^2}(\mat{W}-\mat{W}_0)^T(\mat{W}-\mat{W}_0)\bigg) \\
	\propto\ &\exp\bigg(-\frac{1}{2\sigma^2}(\mat{Y}^T\mat{Y}-2\mat{Y}^T\mat{X}\mat{W}+\mat{W}^T\mat{X}^T\mat{X}\mat{W}) \\
	&\hspace{2.5em}-\frac{1}{2\tau^2}(\mat{W}_0^T\mat{W}_0-2\mat{W}^T\mat{W}_0+\mat{W}^T\mat{W})\bigg)
\end{align*}
We know the posterior will be Gaussian as both the likelihood and prior are so can assume it will take the form: $\exp\big((\mat{W}-\mu)^T\mathbf{\Sigma}^{-1}(\mat{W}-\mu)\big)$. If we multiply the exponent out we get a quadratic so we try and make the posterior we have look like this quadratic.
\begin{align*}
	p(\mat{W}|\mat{X},\mat{Y}) \propto \exp\bigg(&-\frac{1}{2\sigma^2}\mat{W}^T\mat{X}^T\mat{X}\mat{W}-\frac{1}{2\tau^2}\mat{W}^T\mat{W}	&&\mbox{quadratic term}\\
	&+\frac{1}{\sigma^2}\mat{W}^T\mat{X}^T\mat{Y}+\frac{1}{\tau^2}\mat{W}^T\mat{W}_0							&&\mbox{mixed term}\\
	&-\frac{1}{2\sigma^2}\mat{Y}^T\mat{Y}-\frac{1}{2\tau^2}\mat{W}_0^T\mat{W}_0\bigg)							&&\mbox{constant term}\\
\end{align*}
By re-arranging and completing the square we can find both $\mathbf{\Sigma}^{-1}$ and $\mathbf{\Sigma}^{-1}\mu$ in the quadratic and mixed terms respectively.
\begin{align*}
	p(\mat{W}|\mat{X},\mat{Y}) \propto \exp\bigg(&-\frac{1}{2}\mat{W}^T\overbrace{\Big(\frac{1}{\sigma^2}\mat{X}^T\mat{Y}+\frac{1}{\tau^2}\mat{I}\Big)}^{\mathbf{\Sigma}^{-1}}\mat{W}	&&\mbox{quadratic term}\\
	&+\mat{W}^T\underbrace{\Big(\frac{1}{\sigma^2}\mat{X}^T\mat{Y}+\frac{1}{\tau^2}\mat{W}_0\Big)}_{\mathbf{\Sigma}^{-1}\mu}						&&\mbox{mixed term}\\
	&-\frac{1}{2\sigma^2}\mat{Y}^T\mat{Y}-\frac{1}{2\tau^2}\mat{W}_0^T\mat{W}_0\Big)								&&\mbox{constant term}\\
\end{align*}
From these we can easily find $\mathbf{\Sigma}$ and $\mu$.
\begin{align*}
	\mathbf{\Sigma} = &\bigg(\frac{1}{\sigma^2}\mat{X}^T\mat{Y}+\frac{1}{\tau^2}\mat{I}\bigg)^{-1} \\
	\mathbf{\Sigma}^{-1}\mu = &\frac{1}{\sigma^2}\mat{X}^T\mat{Y} + \frac{1}{\tau^2}\mat{W}_0 \\
	\mu = &\mathbf{\Sigma}\bigg(\frac{1}{\sigma^2}\mat{X}^T\mat{Y} + \frac{1}{\tau^2}\mat{W}_0\bigg) \\
\end{align*}
Thus we have our posterior as a Guassian with our values for $\mathbf{\Sigma}$ and $\mu$:
\begin{align*}
	p(\mat{W}|\mat{X},\mat{Y}) \propto\ &\exp\bigg(-\frac{1}{2}(\mat{W}-\mu)^T\Sigma^{-1}(\mat{W}-\mu)\bigg)\\
	\propto\ &\mathcal{N}\bigg[\Big(\frac{1}{\sigma^2}\mat{X}^T\mat{Y}+\frac{1}{\tau^2}\mat{I}\Big)^{-1}\Big(\frac{1}{\sigma^2}\mat{X}^T\mat{Y} + \frac{1}{\tau^2}\mat{W}_0\Big),\Big(\frac{1}{\sigma^2}\mat{X}^T\mat{Y}+\frac{1}{\tau^2}\mat{I}\Big)^{-1}\bigg]
\end{align*}
\subsubsection{Non-parametric regression}
\question Parametric models assume that the distribution the data comes from is based on a finite, fixed set of parameters and models future predictions based off these parameters, they capture everything there is to know about the data. Non-parametric models do not make such assumptions about the model structure and instead infer structure from the data, they have parameters but these are not fixed in advance and there can be an infinite set of parameters.

Non-parametric models are more flexible and can represent a wider variety of data and will represent the data better if the assumptions made in the parametric model are incorrect but are less precise and accurate than parametric methods if the right assumptions are made.

Parametric models are often easier interpreted as they are simpler to transcribe and are also often faster to compute due to lacking the complexity and flexibility of the non-parametric models.
\question As we use a Gaussian process we define this prior over functions and want our prior to put some constraints on the space of functions. The fact that its a Gaussian process means that for an arbitary set of points $x_i, \ldots, x_j$ we assume that $p(f_i), \ldots,f(x_j))$ is jointly Gaussian with mean $0$ and covariance function $k$, from this we have the equation for our prior:
$$
	p(f|\mat{X},\bm{\theta}) = \mathcal{N}(0, k(\mat{X},\mat{X}))
$$
The covariance function $k$ allows us to set constraints from our assumptions about the mapping $f$. For example our assumption about the functions smoothness that if $x_i$ and $x_j$ are similar then we expect $f_i$ and $f_j$ to be similar too, with certain kernel functions for $f$ we can only sample functions that have sufficient smoothness by ensuring that constraint.
\question [TODO]
\question
\begin{align*}
	p(\mat{Y},\mat{X},f,\bm{\theta}) = p(\mat{Y}|f)p(f|\mat{X},\bm{\theta})p(\mat{X})p(\bm{\theta})
\end{align*}
\begin{center}
\begin{tikzpicture}
	\node[obs] (Y) {$\mat{Y}$}; %
	\node[latent, left=of Y] (f) {$f$} ; %
	\node[obs, left=of f, yshift=0.5cm] (X) {$\mat{X}$} ; %
	\node[latent, left=of f, yshift=-0.5cm] (theta) {$\bm{\theta}$} ; %
	\edge {X,theta} {f}; %
	\edge {f} {Y}; %
\end{tikzpicture}
\end{center}
[UNSURE]
\begin{itemize}
	\item $\mat{X}$ and $\bm{\theta}$ are independent
	\item $f$ is conditionally dependent on both $\mat{X}$ and $\bm{\theta}$
	\item $\mat{Y}$ is conditionally dependent on $f$ and conditionally independent of $\mat{X}$ and $\bm{\theta}$
\end{itemize}
\question [UNSURE] This marginalization shows the likelihood of the data we have observed over the function we are testing which is constrained constrained by the prior.

There are two sources of uncertainty here, that associated with $f$ in the prior and that associated with $\epsilon$ in the likelihood, these are independent and as such are merged by simply adding to form the covariance of the marginal likelihood Gaussian.

Leaving the $\theta$ on the left-hand side of the expression implies that we still have specific hyperparameters rather than undefined ones. It remains throughout the integral.
\subsection{Practical}
\subsubsection{Linear regression}
\question [TODO]
\subsubsection{Non-parametric regression}
\question [TODO]
\question [TODO]
\section{The posterior}
\subsection{Theory}
\question Specifiying a prior assumption over \mat{X} allows us to encode our preference about the nature of the properties that \mat{X} should have. This assumption also constrains the variable \mat{W} as \mat{W} and \mat{X} have a simple relationship.
\question We have encoded the assumption that all the dimensions in each variable of $\mat{X}$ are independent and identically distributed as we have used a Gaussian with an identity matrix as its covariance matrix.
\question [TODO]
\subsubsection{Learning}
\question [TODO]
\subsubsection{Practical optimisation}
\question \emph{1.} We have from Question 17 that:
\begin{align*}
	p(\mat{Y}|\mat{W}) = \prod_{i=1}^N\mathcal{N}(y_i|0,\mat{W}\mat{W}^T + \sigma^2\mat{I})
\end{align*}
then the objective function is derived as follows using log rules:
\begin{align*}
	\mathcal{L}(\mat{W}) &= -\log\bigg(\prod_{i=1}^N\mathcal{N}(y_i|0,\mat{W}\mat{W}^T + \sigma^2\mat{I})\bigg) \\
	&= -\sum_{i=1}^N\log\big(\mathcal{N}(y_i|0,\mat{W}\mat{W}^T + \sigma^2\mat{I})\big) \\
	&= -\sum_{i=1}^N\log\bigg(\frac{1}{\sqrt{2\pi\lvert\Sigma\rvert}}\bigg)+\log\bigg(\exp\Big(-\frac{1}{2}y_i^T\Sigma^{-1} y_i\Big)\bigg) \\
	&= -\sum_{i=1}^N\log\frac{1}{(2\pi)^{\frac{D}{2}}\lvert\mat{W}\mat{W}^T+\sigma^2\mat{I}\rvert^{\frac{1}{2}}}+\log\bigg(\exp\Big(-\frac{1}{2}y_i^T(\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1} y_i\Big)\bigg) \\
	&= -\log\big((2\pi)^D\lvert\mat{W}\mat{W}^T+\sigma^2\mat{I}\vert\big)^{-\frac{N}{2}} - \sum_{i=1}^N-\frac{1}{2}y_i^T(\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1} y_i \\
	&= \frac{N}{2}\bigg(D\log2\pi + \log(\lvert\mat{W}\mat{W}^T+\sigma^2\mat{I}\vert)\bigg) + \frac{1}{2}\sum_{i=1}^Ny_i^T(\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1} y_i \\
\end{align*}
From the fact that $\Tr(\mat{A}\mat{B}) = \Tr(\mat{B}\mat{A})$ and $\Tr(\mat{C} = \mat{c})$ for the dimensions in our sum we can evaluate it as follows:
\begin{align*}
	\mathcal{L}(\mat{W}) &= \frac{N}{2}\bigg(D\log2\pi + \log(\lvert\mat{W}\mat{W}^T+\sigma^2\mat{I}\vert) + \Tr\big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\mat{Y}\mat{Y}^T\big)\bigg)
\end{align*}
\emph{2.} To find the gradient of $\mathcal{L}$ we will look at each term in turn:
\begin{align*}
	\mathcal{L}(\mat{W}) &= \frac{N}{2}\bigg(\overbrace{\Tr\big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\mat{Y}\mat{Y}^T\big)}^{A} + \overbrace{\log(\lvert\mat{W}\mat{W}^T+\sigma^2\mat{I}\vert)}^{B} + \overbrace{D\log2\pi}^{C}\bigg)
\end{align*}
$C$ is a constant term so we shall discard that. To find the derivative of $B$ we use the rules that $\partial(\log(\det(\mat{X}))) = \Tr(\mat{X}^{-1}\partial\mat{X})$ and that $\sigma^2\mat{I}$ is constant with respect to $\mat{W}$ and so evaluate $B$ to:
\begin{align*}
	\frac{\partial B}{\partial\mat{W}} &= \Tr\Big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\partial(\mat{W}\mat{W}^T)\Big)
\end{align*}
We also have the rules that $\partial(\mat{X}\mat{Y}) = (\partial\mat{X})\mat{Y} + \mat{X}(\partial\mat{Y})$ and that $\frac{\partial\mat{X}}{\partial\mat{X}_{ij}} = \mat{J}^{ij}$ where $\mat{J}$ is the single-entry matrix, having $1$ at $(i, j)$ and $0$ elsewhere and so can reduce $B$ to:
\begin{align*}
	\frac{\partial B}{\partial\mat{W}_{ij}} &= \Tr\Big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}(\mat{J}^{ij}\mat{W}^T + \mat{W}\mat{J}^{ij}^T)\Big)
\end{align*}
Using the earlier rules along with the rule that $\partial(\Tr(\mat{X}))=\Tr(\partial\mat{X})$ we can find the derivative of $A$:
\begin{align*}
	\frac{\partial A}{\partial\mat{W}} &= \Tr\bigg(\partial\Big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\mat{Y}\mat{Y}^T\Big)\bigg) \\
	&= \Tr\Bigg(\partial\Big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\Big)\mat{Y}\mat{Y}^T + (\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\Big(\partial(\mat{Y}\mat{Y}^T)\Big)\bigg)
\end{align*}
As $\partial(\mat{Y}\mat{Y}^T) = 0$ and $\partial(\mat{X}^{-1}) = -\mat{X}^{-1}(\partial\mat{X})\mat{X}^{-1}$ we can reduce $A$ to:
\begin{align*}
	\frac{\partial A}{\partial\mat{W}} &= \Tr\Bigg(\partial\Big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}\Big)\mat{Y}\mat{Y}^T\Bigg) \\
	\frac{\partial B}{\partial\mat{W}_{ij}} &= \Tr\Bigg(\Big(-(\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}(\mat{J}^{ij}\mat{W}^T + \mat{W}\mat{J}^{ij}^T)(\mat{W}\mat{W}^T+\sigma^2\mat{I})\Big)\mat{Y}\mat{Y}^T\Bigg)
\end{align*}
And so by combining our terms $A$ and $B$ we have the gradient for our log likelihood $\mathcal{L}$:
\begin{align*}
	\frac{\partial\mathcal{L}}{\partial\mat{W}_{ij}} =\ &\frac{N}{2}\Tr\Bigg(\Big(-(\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}(\mat{J}^{ij}\mat{W}^T + \mat{W}\mat{J}^{ij}^T)(\mat{W}\mat{W}^T+\sigma^2\mat{I})\Big)\mat{Y}\mat{Y}^T\Bigg) \\
	&+ \frac{N}{2}\Tr\Big((\mat{W}\mat{W}^T+\sigma^2\mat{I})^{-1}(\mat{J}^{ij}\mat{W}^T + \mat{W}\mat{J}^{ij}^T)\Big)
\end{align*}
\subsubsection{Non-parametric}
\question [TODO]
\subsection{Practical}
\subsubsection{Linear representation learning}
\question We minimised the objective function to get our maximum likelihood estimate for $\mat{W}$
\begin{align*}
	\mat{W}^\prime = \argmin_\mat{W} \mathcal{L}(\mat{W})
\end{align*}
To find the representation for $\mat{X}^\prime$ we must know the reverse of the linear mapping.
\begin{align*}
	\mat{Y} &= \mat{X}^\prime\mat{W}^\prime^T \\
	\mat{X}^\prime &= \mat{Y}\mat{W}^\prime(\mat{W}^\prime^T\mat{W}^\prime)^{-1}
\end{align*}
And then use minimization of our objective function using the non-linear conjugate gradient method to find $\mat{X}^\prime$.
\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[width=\linewidth]{oldX}
		\caption{Original $\mat{X}^\prime$}
		\label{fig:oldX}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[width=\linewidth]{newX}
		\caption{Learned $\mat{X}^\prime$}
		\label{fig:newX}
	\end{subfigure}
	\caption{Representation Learning of $\mat{X}^\prime$}
\end{figure} \\
Our learned representation of $\mat{X}^\prime$ is very close to the correct shape only rotated slightly, this is because the marginal likelihood we are maximising is invariant to any matrix transformation whose inverse is its own transposition i.e. any matrix for which $\mat{R}\mat{R}^T = \mat{I}$ (rotations along with a few other transformations fall into this category). Therefore any of these transformations can be applied to $\mat{W}$ without the marginal likelihood changing. We can see this by setting $\mat{W}$ in the likelihood to $\mat{W}^\prime\mat{R}$:
\begin{align*}
	\mathcal{L}(\mat{W}) &= -\sum_{i=1}^N\log\big(\mathcal{N}(y_i|0,(\mat{W}^\prime\mat{R})(\mat{W}^\prime\mat{R})^T + \sigma^2\mat{I})\big) \\
	&= -\sum_{i=1}^N\log\big(\mathcal{N}(y_i|0,\mat{W}^\prime\mat{R}\mat{R}^T\mat{W}^\prime^T + \sigma^2\mat{I})\big) \\
	&= -\sum_{i=1}^N\log\big(\mathcal{N}(y_i|0,\mat{W}^\prime\mat{W}^\prime^T + \sigma^2\mat{I})\big) \\
\end{align*}
\end{document}
